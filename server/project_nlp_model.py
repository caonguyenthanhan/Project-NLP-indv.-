# -*- coding: utf-8 -*-
"""project_nlp_modele.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3WmzoLTQc6dSEmsBGwODxTHCpF8wsih
"""

import pandas as pd
import numpy as np
import joblib
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import os

# Hàm tải dataset từ Hugging Face

def load_dataset_from_hf(dataset_name, split='train'):
    dataset = load_dataset(dataset_name, split=split)
    return pd.DataFrame(dataset)

# Hàm hỗ trợ

def preprocess_and_train(data, text_column, label_column, task_name):
    label_counts = data[label_column].value_counts()
    if len(label_counts) < 2:
        print(f"Cảnh báo: Dữ liệu '{task_name}' chỉ chứa một lớp, cần kiểm tra lại!")
        return {}, None, {}

    X_train, X_test, y_train, y_test = train_test_split(
        data[text_column], data[label_column], test_size=0.2, random_state=42
    )

    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)

    models = {
        'Naive Bayes': MultinomialNB(),
        'Logistic Regression': LogisticRegression(max_iter=1000),
        'SVM': LinearSVC(max_iter=1000)
    }

    results = {}
    for name, model in models.items():
        model.fit(X_train_tfidf, y_train)
        y_pred = model.predict(X_test_tfidf)
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = accuracy
        print(f'{task_name} - {name} Accuracy: {accuracy:.4f}')

    return results, vectorizer, models

def plot_results(results_dict):
    fig, ax = plt.subplots(figsize=(10, 6))
    for dataset, accuracies in results_dict.items():
        ax.plot(list(accuracies.keys()), list(accuracies.values()), marker='o', label=dataset)
    ax.set_title('So sánh Độ chính xác của các Mô hình trên các Tập dữ liệu')
    ax.set_xlabel('Mô hình')
    ax.set_ylabel('Độ chính xác')
    ax.legend()
    plt.grid(True)
    plt.savefig(os.path.join(MODEL_DIR, 'model_comparison.png'))
    plt.show()

# Tải dữ liệu từ Hugging Face
imdb_df = load_dataset_from_hf('imdb')
imdb_results, imdb_vectorizer, imdb_models = preprocess_and_train(imdb_df, 'text', 'label', 'IMDB Reviews')

ag_news_df = load_dataset_from_hf('ag_news')
ag_news_results, ag_news_vectorizer, ag_news_models = preprocess_and_train(ag_news_df, 'text', 'label', 'AG News')

twitter_df = load_dataset_from_hf('sentiment140', split='train[:50000]')
twitter_df['label'] = twitter_df['sentiment'].map({0: 0, 4: 1})
twitter_results, twitter_vectorizer, twitter_models = preprocess_and_train(twitter_df, 'text', 'label', 'Twitter Sentiment')

sms_df = load_dataset_from_hf('sms_spam')
sms_results, sms_vectorizer, sms_models = preprocess_and_train(sms_df, 'sms', 'label', 'SMS Spam')

bbc_df = load_dataset_from_hf('ag_news')
bbc_df['label'] = bbc_df['label'].factorize()[0]
bbc_results, bbc_vectorizer, bbc_models = preprocess_and_train(bbc_df, 'text', 'label', 'BBC News')

yelp_df = load_dataset_from_hf('yelp_review_full', split='train[:10000]')
yelp_results, yelp_vectorizer, yelp_models = preprocess_and_train(yelp_df, 'text', 'label', 'Yelp Reviews')

# Tạo thư mục lưu mô hình
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# Lưu mô hình và vectorizer vào thư mục models
def save_models(models, vectorizer, name):
    joblib.dump(vectorizer, os.path.join(MODEL_DIR, f'{name}_vectorizer.pkl'))
    for model_name, model in models.items():
        joblib.dump(model, os.path.join(MODEL_DIR, f'{name}_{model_name}.pkl'))

save_models(imdb_models, imdb_vectorizer, 'IMDB_Reviews')
save_models(ag_news_models, ag_news_vectorizer, 'AG_News')
save_models(twitter_models, twitter_vectorizer, 'Twitter_Sentiment')
save_models(sms_models, sms_vectorizer, 'SMS_Spam')
save_models(bbc_models, bbc_vectorizer, 'BBC_News')
save_models(yelp_models, yelp_vectorizer, 'Yelp_Reviews')

# So sánh kết quả
results_dict = {
    'IMDB Reviews': imdb_results,
    'AG News': ag_news_results,
    'Twitter Sentiment': twitter_results,
    'SMS Spam': sms_results,
    'BBC News': bbc_results,
    'Yelp Reviews': yelp_results
}

for dataset, accuracies in results_dict.items():
    print(f'\n{dataset}:')
    for model, acc in accuracies.items():
        print(f'  {model}: {acc:.4f}')

plot_results(results_dict)

import shutil

# Nén thư mục models thành một file zip
shutil.make_archive("models", 'zip', "models")

# Tải về từ Colab
from google.colab import files
files.download("models.zip")